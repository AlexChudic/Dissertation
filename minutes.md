# Summaries of the supervisor meetings

## Meeting 1 - 19.9.2023
### Minutes
* Should think about
  * What LLM to use 
  * Which repo to use
  * What are the determining metrics for assessing refactorings (suggestion: SonarQube, M. Fowler book)
  * How to do prompt engineering
  
### Project Workflow
1. Select LLM
2. Understand their training data
3. Setup a baseline on how to evaluate code refactoring
4. Mine GitHub repos to see the developer code refactorings


## Meeting 2 - 29.9.2023
### Minutes
* Talked about some interesting papers for research - https://arxiv.org/pdf/2305.04764.pdf
* Should look at the codebase or related papers
* Setup Zotero and share GitHub repo


## Meeting 3 - 13.10.2023
### Minutes
* Discussion about LLMs - decided to use CodeLlama as ChatGPT 3.5 is paid
* Next we discussed the overlap between the LLM training dataset and the evaluation set of the research.
  * It shouldn't overlap as it would decrease the value of the research
  * Should research how it's done in related literature (e.g. https://arxiv.org/abs/2305.01210 )
* Talked about refactorings
  * Refactoring Miner - useful tool, should play around with it
  * It is advised to first start with the simple refactorings and make sure the pipeline works and later build on that
  * Think about the type of refactorings I want to test first
    * For refactorings that affect functionality outside the scope the call graph might be useful
    * https://github.com/gousiosg/java-callgraph
  * Look at the book by M.Folwer as it might provide some guidance

###  Next steps
  1. Think about how to curate the dataset, which consists of open-source software projects
     *  Which open-source repo to use? (GitLab, GitHub, BitBucket)
     *  Play with RefactoringMiner, and see how it works
  2. Think about how one should give input the code into Llma so that Llma can refactor it


## Meeting 4 - 20.10.2023
### Minutes
* We talked about which projects to use for the refactoring dataset, and how the ChatUniTest paper mentions 5/10 projects it used were not in the training dataset. I will try to go with the same 50/50 ratio.
* Might be a good idea to include the Abstract Syntax Tree representation of the class in the prompt
* Should start simple and then scale up the whole process. Will be nice to start with some manual LLM refactoring testing 

###  Next steps
  1. Make a small dataset of simple refactoring (from tutorials etc.) with the refactored code
  2. Manually prompt these examples to an LLM to ask for refactoring. Take note of the prompt and results. Start with simple prompts - “Refactor the following code..”. Improve the prompt iteratively, applying prompt patterns and further specifying the requirements
  3. Produce a document that explains the steps taken and shows the input prompts with the output from the LLM

## Meeting 5 - 02.11.2023
### Minutes
* I have gone through the document I created on manual prompt refactoring and we discussed it
  * I should clearly explain the steps I have taken with support of papers etc.
* We talked about the metrics useful for the Evaluation function
  * Complexity - primarily cognitive complexity (readability)
  * Maintainability
  * Normalised edit distance - how different is the code generated by humans and LLM
  * Other?
    * Technical debt?
    * Does LLM refactoring break the code or introduce any vulnerabilities?
  * RQ1: How good LLMs are in code refactoring compared to humans?
  * RQ1.1: Do LLMs change the functionality of the code or introduce bugs?

###  Next steps
  1. Evaluate if the LLM refactoring can achieve similar results to what is done in https://github.com/guydunton/Refactoring-example-cpp
  2. Possibly do it by mining commits with Refactoring Miner
  3. Think about metrics to use in the Evaluation function


## Meeting 6 - 17.11.2023
### Minutes
 * Need to be more rigorous and systematic in the research
   * Support all the decisions with some data
   * Look at the semantic differences between the code generated by LLM and the developer
 * Detect the parts where the code has been refactored and follow that with detecting parts where other code had to be changed because of that
 * Don't focus on coding but rather formulating the research questions well and then systematically follow from there


###  Next steps
   1. Look at normalized edit distance between the refactored code
   2. Think about metrics to use in the Evaluation function
      * Research SonarCube
   3. Create a set of tutorials from git with answers
      * Better than looking at high-scale projects, as they can easily fit into the LLM context limit

## Meeting 7 - 24.11.2023
- When evaluating the refactorings, do not try to get one value, rather look at different metrics in which the LLM performs better
    - Make a table
- Two ways in which I can look at the refactorings:
    1. Give LLM only the lines of code where the refactoring has been performed
    2. Give LLM the whole file where the refactoring has been performed, and analyse the overlap between the two

###  Next steps
  1. Use the refactoring miner on the Tutorial project (one provided in refactoring miner)
  2. Find the exact file and lines where the refactoring is performed - possibly try using the LLM to try and figure out what needs to be refactored
     -  **RQ1: Can LLM detect what type of refactoring needs to be done on the code**
        - Input: lines of code where the refactoring is performed
        - Question: What type of refactoring needs to be done on the given code?
            - Try applying some prompt engineering patterns
        - Ground Truth?: the refactoring done by the Refactoring miner
        - Compare the result to what type has the Refactoring miner detected
            - Careful, RM might not be correct
  3. Ask the LLM to perform the refactoring itself
     - **RQ2: How good LLMs are in code refactoring compared to humans (with or without tools)?**
        - Input: lines of code where the refactoring is performed
        - Question: Refactor the following code…
            - TODO Look at formulating the prompt later
        - Ground Truth??: The refactoring done by the developer
        - Compare the refactorings - important to consider the developer refactoring might be done suboptimally (or more refactoring is needed)
  4. Make an evaluation matrix for the important metrics

## Meeting 8 - 22.12.2023
- We talked about the progress on the evaluation pipeline
- Should focus on finishing the pipeline so I can evaluate the results and establish an understanding
- Received some interesting resources I should look at

## Meeting 9 - 19.1.2024
- Talked about the progress in the pipeline - all parts pretty much finished, just trying to link them together
    - Problem with ChatGPT -> should try Llama hosted by the university
- Evaluation will be looking at the differences between the
    - LLM refactoring eval - Previous eval
    - Developer refactoring eval - Previous eval
- Look at other prompt engineering papers, guidelines and ideas
    - RQ2: Does prompt engineering lead to better results?
- Important Detailed README file to be able to replicate what I did
- A brief description of the code is enough (some diagram might be nice)

###  Next steps
  1. Complete the pipeline
     - Make ChatGPT work and/or try using the Llama model hosted by the university
     - Look at the evaluation metrics for the simple prompts
  3. Look at papers about prompt engineering and test other prompt designs
  4. Start writing down some ideas for the dissertation

## Meeting 10 - 26.1.2024
- Talked about the progress in the pipeline
    - Made ChatGPT Work
    - Worked on automatic evaluation
- First continue with the simple experiment - only on single file refactorings, will try multiple file refactorings later
    - Make sure to get 
    - Langchain might be useful for the multiple file refactoring 

###  Next steps
  1. Complete the experiment only considering the refactoring that is in a single file (i.e., cases where there is only a single refactoring in that file)
     - In that case, you will first ask LLM to refactor the code, because we want to answer the following RQ: Can LLMs detect the type of refactoring they must do?
     - In the second experiment you can give the "description" of the Refactoring miner as the prompt.
  2. Try to play with Langchain, and see if you can use it for refactoring that affects multiple files.
  3. Try collecting the dataset to use for evaluation afterward
     - Perform the whole single-file experiment on some of them

## Meeting 11 - 2.2.2024
- Finished the initial version of the pipeline
    - The results look pretty decent - table and chart
- Talked about how to choose the repositories for the evaluation to make sure the sample is representative of the population
- The long evaluation computation time and minor cost associated with running the pipeline
    - Might be okay to only take a sample (1000?) of commits for every repository
- Important to write a good readMe so that the pipeline is reproducible


## Meeting 12 - 9.2.2024
- Looked at the representative sample size
- Is the GitHub repo star count a good metric to consider for creating distribution?

## Meeting 13 - 16.2.2024
- Summarizing the progress, describing the sampling process of the repositories
- Talking about what prompts to use
    - Is specifying the rows of the code where refactoring miner is performed appropriately? Is there any practical use case?
    - Yes - the software developer is working on functionality in a section of the class (i.e. a method) and wants to check whether any refactorings can be performed.
- Temperature discussion
    - For the default temperature (0.7) each prompt should be run multiple times (10ish) and the output variance needs to be considered, introducing new evaluation metrics
    - For the lower temperature (e.g. 0.2) it might not be necessary to run each prompt many times, making it more convenient
 
###  Next steps
  1. Complete the experiment only considering the refactoring that is in a single file (i.e., cases where there is only a single refactoring in that file)
     - Experiment with the temperature - see if 0.2 temp gives reasonable refactorings
  2. Estimate the costs of running it on the pipeline on 62 repos
  3. Run the pipeline on all of the sampled repos


