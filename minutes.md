# Summaries of the supervisor meetings

## Meeting 1 - 19.9.2023
### Minutes
* Should think about
  * What LLM to use 
  * Which repo to use
  * What are the determining metrics for assessing refactorings (suggestion: SonarQube, M. Fowler book)
  * How to do prompt engineering
  
### Project Workflow
1. Select LLM
2. Understand their training data
3. Setup a baseline on how to evaluate code refactoring
4. Mine GitHub repos to see the developer code refactorings


## Meeting 2 - 29.9.2023
### Minutes
* Talked about some interesting papers for research - https://arxiv.org/pdf/2305.04764.pdf
* Should look at the codebase or related papers
* Setup Zotero and share GitHub repo


## Meeting 3 - 13.10.2023
### Minutes
* Discussion about LLMs - decided to use CodeLlama as ChatGPT 3.5 is paid
* Next we discussed the overlap between the LLM training dataset and the evaluation set of the research.
  * It shouldn't overlap as it would decrease the value of the research
  * Should research how it's done in related literature (e.g. https://arxiv.org/abs/2305.01210 )
* Talked about refactorings
  * Refactoring Miner - useful tool, should play around with it
  * It is advised to first start with the simple refactorings and make sure the pipeline works and later build on that
  * Think about the type of refactorings I want to test first
    * For refactorings that affect functionality outside the scope the call graph might be useful
    * https://github.com/gousiosg/java-callgraph
  * Look at the book by M.Folwer as it might provide some guidance

###  Next steps
  1. Think about how to curate the dataset, which consists of open-source software projects
     *  Which open-source repo to use? (GitLab, GitHub, BitBucket)
     *  Play with RefactoringMiner, and see how it works
  2. Think about how one should give input the code into Llma so that Llma can refactor it


## Meeting 4 - 20.10.2023
### Minutes
* We talked about which projects to use for the refactoring dataset, and how the ChatUniTest paper mentions 5/10 projects it used were not in the training dataset. I will try to go with the same 50/50 ratio.
* Might be a good idea to include the Abstract Syntax Tree representation of the class in the prompt
* Should start simple and then scale up the whole process. Will be nice to start with some manual LLM refactoring testing 

###  Next steps
  1. Make a small dataset of simple refactoring (from tutorials etc.) with the refactored code
  2. Manually prompt these examples to an LLM to ask for refactoring. Take note of the prompt and results. Start with simple prompts - “Refactor the following code..”. Improve the prompt iteratively, applying prompt patterns and further specifying the requirements
  3. Produce a document that explains the steps taken and shows the input prompts with the output from the LLM

## Meeting 5 - 02.11.2023
### Minutes
* I have gone through the document I created on manual prompt refactoring and we discussed it
  * I should clearly explain the steps I have taken with support of papers etc.
* We talked about the metrics useful for the Evaluation function
  * Complexity - primarily cognitive complexity (readability)
  * Maintainability
  * Normalised edit distance - how different is the code generated by humans and LLM
  * Other?
    * Technical debt?
    * Does LLM refactoring break the code or introduce any vulnerabilities?
  * RQ1: How good LLMs are in code refactoring compared to humans?
  * RQ1.1: Do LLMs change the functionality of the code or introduce bugs?

###  Next steps
  1. Evaluate if the LLM refactoring can achieve similar results to what is done in https://github.com/guydunton/Refactoring-example-cpp
  2. Possibly do it by mining commits with Refactoring Miner
  3. Think about metrics to use in the Evaluation function
